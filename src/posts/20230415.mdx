---
id: "20230415"
slug: "/20230415"
title: "Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"
description: ""
author: "Vive Kang"
date: "2023-04-15"
image: ""
tags: ["NLP", "paper review"]

---
## Motivation & Introduction

BERT 같은 pre-trained LM이 여러 task에서 좋은 성능을 보이고 있고, 최근 연구에서는 pre-trained LM이 commonsense knowledge도 갖고 있다는 걸 밝혀냈다. 그러나 pre-trained LM이 numerical commonsense knowledge에 대해서 좋은 성능을 보이지 못한다는 걸 발견했다.

![0](../blogImage/27-0.png)

그래서 이 논문에서는 pre-trained LM이 numerical commonsense knowledge(entity 간의 numeric relation에 대한 commonsense knowledge)를 잡아낼 수 있는지에 대한 연구를 진행한다. 이를 측정하기 위한 방법으로 **masked-word-prediction based probing task**를 제안한다. 이 task에서는 마스킹된 곳에 들어갈 numeric word의 ranking을 통해 pre-trained LM의 numeric commonsense knowledge를 평가한다. 

이런 내용을 바탕으로 8개의 카테고리에서 3145개의 질문(probe)을 가진 NumerSense dataset을 만들었다. 초기 실험에서 pre-trained LM이 adversarial attack에 취약하다는 걸 발견했다.

- adversarial attack
    
    모델이 부적절한 결과를 내도록 input 값을 조작하는 것.
    

Figure 1에서 아래 두개를 비교해보면 처음에는 "four"라는 numeric word를 잘 예측했지만 input을 살짝만 바꿔주자(단순히 "round"만 추가했을 뿐인데) 틀린 답을 내놓았다. 따라서 의도적으로 adversarial example들을 포함시켜서 probe를 구성했다. 

pre-trained LM을 두가지 세팅으로 평가했다. (1) NumerSense로 fine-tuning하지 않은 zero-shot setting, (2) 모델을 연관된 commonsense reasoning dataset의 example로부터 fine-tuning하는 distant supervision setting. 결과적으로 masked-word-prediction based probing task에 대해서 사람보다 pre-trained LM이 한참 낮은 성능을 보였다(distant supervision이 어느정도 도움이 되기는 했지만 그래도 성능이 낮았다). 

이 논문이 (1) pre-trained LM의 numerial commonsense에 대한 능력을 높이고, (2) 기존의 commonsense knowledge base에 numerical fact의 양을 늘리고, (3) "Q: How many legs do ants have?" "A: Six!" 같은 Open-domain QA에 기여하기를 원한다.

## The NumerSense Probing Task

numerical commonsense reasoning probing task를 소개하고 NumerSense dataset을 만드는 과정도 소개한다. 그 후 어떤 종류의 knowledge를 probe를 통해 얻을 수 있는지에 대한 분석을 제공하고 마지막으로 fine-tuning이 성능을 높일 수 있는지 보기 위해 추가로 high-quality distant supervision을 사용한다.

### Task Formulation

pre-trained LM을 마스킹 토큰을 채우도록 하고 그 output distribution을 통해 분석을 했다. ranking이 numerical commonsense knowledge를 나타내고, highest ranked number word가 정답일 경우 그 probe는 성공적인 것으로 여긴다. 각 probe에서 마스킹 위치는 numeric word가 빈칸을 채울 가능성이 높도록 선택한다.

### Probing Data Collection

기존 corpus Open Mind Common Sense(OMCS)를 사용해 dataset을 만들었다. 먼저 OMCS에서 다음 12가지 number word를 포함하는 문장을 선별했다.  \{ "no", "zero", "one", "two", ..., "ten" \}. 

그러나 예상대로 많은 noisy statement가 있었는데, 부정확하고 오타를 포함하고 numerical commonsense logic을 갖고 있지 않은 경우가 있었다. 따라서 대학원생들에게 모두 accept된 statement만 남기는 과정을 2번 진행했다. 결과적으로 1131개의 cleaned statement를 얻을 수 있었다.

초기 실험을 통해 masked number word 근처에 형용사 하나를 추가하는 것 만으로도 pre-trained LM이 제대로 동작하지 않는다는 걸 발견했다. 따라서 모델의 그 성능을 평가하기 위해, dataset에 adversarial example들(numeric word의 대상이 되는 noun 앞에 형용사를 추가한 example)을 넣었다. 형용사 후보들은 commonsense KG(ConceptNet)에서 관련된 triple을 통해 생성되었다(e.g. Figure 1의 예시에선 \< Wheel, HasProperty, round \>). 그리고 human annotator가 해당 adversarial example들이 적절하고 자연스럽도록 수정 작업을 거친다. 최종적으로 3145개의 NumerSense probe(adversarial examples)를 모았다.

각 probe에 8종류의 카테고리 라벨링을 통해 어떤 주제가 얼만큼 포함되어 있는지 전체적으로 좀 더 이해하기 쉽게끔 했다.

![1](../blogImage/27-1.png)

### Supervision for Fine-tuning PTLMs

논문의 task로 fine-tuning 했을 때 성능을 높일 수 있는지 알아보기 위해 GenericsKB로부터 training sentence를 모았다. 이 sentence를 적절히 사용한다면 PTLM의 numerical commonsense knowledege에 대한 성능을 높일 수 있을 거라 기대한다.

### Statistics of NumerSense

![2](../blogImage/27-2.png)

![3](../blogImage/27-3.png)

12개 항목의 정답 number word의 분포를 나타낸 그래프이다. 

## Empirical Analysis

experiment set-up과 여러 PTLM의 zero-shot setting, distantly supervised fine-tuning setting 에서의 결과를 보여준다. 

### Experiment Set-up

1. **zero-shot inference**
PTLM을 어떤 수정도 없이 그대로 분석한다.
2. **additional supervision via fine-tuning**
앞서 Supervision for Fine-Tuning PTLMs 파트에서 모든 데이터 sentence의 number word에 마스킹을 한다. 그리고 이 데이터로 fine-tuning한 후 NumerSense로 평가한다.

### Evaluation Metric and Human Bound

masked-word-prediction의 결과로 (fine-tuning 여부에 상관없이) 모든 vocabulary에 대한 확률 distribution을 출력한다. 앞서 말했듯 NumerSense는 이 distribution을 통해 모든 number word의 순위를 매기고 평가한다. 평가할 때 hit@1/2/3 accuracy metric을 사용하는데, 이는 정답 number word가 top k number word에 속해있는지를 계산하는 지표이다.

동일한 task에 대해 human performance도 측정했다. 두 그룹으로 나누어서 한 그룹은 open-book test(외부 정보 이용 가능)로, 다른 그룹은 closed-book test으로 진행했고 다수의 라벨을 받은 결과를 human label의 최종 결과로 이용했다.

### Experimental results

![4](../blogImage/27-4.png)

2번째 block은 4개의 PTLM에서 zero-shot inference setting의 결과이다. 모델의 크기가 커질수록 그에 따라 성능이 증가함을 보인다. 그리고 RoBERTa가 BERT보다는 좋은 성능을 보이는데 이는 RoBERTa가 더 많은 pre-training을 했기 때문일 것이라고 한다. 

fine-tuning이 성능 향상에 도움이 된다는 걸 알 수 있다. 그러나 아직 human closed-book test에도 한참 미치지 못한다. 아래의 Figure 4는 PTLM들이 NumerSense의 모든 카테고리에서 낮은 성능을 보인다는 걸 나타낸다.

![5](../blogImage/27-5.png)

1131개의 "Core Probe" set과 3145개의 "Adversarial examples" set에서의 성능을 비교해보았을 때 모든 PTLM이 adversarial set에서 성능이 훨씬 낮았다. 이는 PTLM이 (fine-tuning이 되었더라도) adversarial attack에 당하기 쉽다는 걸 뜻하고, 앞으로의 PTLM에 대한 방향은 contextual representation을 학습할 때 dependency나 semantic role 같은 더 구조적인 inductive biases를 고려해야 한다는 걸 의미한다.

## Case Studies

**Object bias**

앞서 들었던 예시 중에 "a bird usually has [MASK] legs"에서 BERT-large 모델은 "four"라는 답을 내놓았다. 여기서 [MASK] 뒤에 오는 단어가 "legs"라면 항상 "four"라고 예측할까? 이런 bias가 있는지 알아보기 위해 몇가지 case study를 진행했다(Table 3).

![6](../blogImage/27-6.png)

[x]에는 1000개의 다른 임의로 생성된 단어를 넣었다. BERT와 RoBERTa 둘 다 특정 정답에 대한 bias가 있는 걸로 보인다. 그리고 RoBERTa의 수정된 pre-training 방식이 bias를 줄이는 데 도움이 된 것으로 보인다. 

**Attention distribution**

![7](../blogImage/27-7.png)

"A bird usually has two legs"라는 문장에서 "two"에 대한 attention distribution plot이다. 12-layer, 12-head로 총 x는 144까지 나타냈다. numerical commonsense에 중요한 단어인 "birds"나 "legs"는 쭉 낮은 attention weight를 보였는데, 이는 BERT/RoBERTa가 subject/object와 number word간의 관계를 잃어버린다는 걸 뜻한다.

## Open-Domain ‘How-Many’ Questions

NumerSense의 example들은 ‘how-many’에 초점을 맞추면 open-domain question으로도 볼 수 있다. 따라서 추가적으로 NumerSense에 대한 sota open-domain QA 모델의 성능을 확인해보았다.

Natural Question(NQ)로 학습된 모델을 사용했고, NumerSense의 [MASK] 부분을 'how many'로 바꿔줘서 probe가 NQ example과 비슷하게 만들었다. 예를 들어, "a fly usually has [MASK] legs"를 "how many legs a fly usually has?"로 바꾸었다. sota 모델에서 15.4%의 BERT-base보다 낮은 성능이 나왔는데, 이로써 NumerSense에서의 성능을 높이는 게 open-domain 'how-many' QA의 성능을 높여줄 수 있다는 걸 보여줬다.

## Conclusion

PTLM에서 numeric commonsense knowledge를 유도하기 위한 probing task로 NumerSense를 제안한다. 강력한 PTLM인 BERT나 RoBERTa 조차도 낮은 성능을 보였고, 심지어 high-quality distant supervision으로 fine-tuning하고 나서도 성능이 한참 낮았다. 이런 발견과 NumerSense가 pre-trained masked LM의 numerical commonsense knowledge에 대한 성능을 높여줄 것이라 기대한다.

---

피드백은 언제나 환영합니다. 잘못된 내용이 있으면 댓글로 피드백 부탁드립니다.
