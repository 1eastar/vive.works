---
id: "20230430"
slug: "/20230430"
title: "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
description: ""
author: "Vive Kang"
date: "2023-04-30"
image: ""
tags: ["NLP", "paper review"]

---
## Motivation & Introduction

sentence embedding은 NLP 영역에서 중요한 부분이다. 이 논문에서는 sota sentence embedding method를 발전시키고 contrastive objective가 pre-trained LM과 함께 사용되었을 때 굉장히 효율적이라는 것을 보여주려 한다. 그리고 unlabeled/labeled data 둘 다로부터 뛰어난 sentence embedding을 만들어낼 수 있는 SimCSE를 제안한다.

![0](../blogImage/32-0.png)

unsupervised SimCSE에서는 input sentence에 noise를 주기 위해 dropout을 사용한다. 즉, 같은 sentence를 pre-trained encoder에 두번 집어넣어서 두개의 다른 embedding 값인 "positive pair"를 얻을 수 있다. 그리고 in-batch negative를 "negatives"로 두고 모델에게 negative들 사이에서 positive를 예측하게 한다. 굉장히 단순한 방법이지만 NSP나 discrete data augmentation(e.g. word deletion, replacement …) 같은 training objective보다 훨씬 성능이 뛰어났다. 여기서 dropout이 최소한의 data augmentation 의 역할을 한다는 것을 발견했고 representation collapse도 막아준다는 걸 발견했다.

supervised SimCSE에서는 NLI dataset을 이용한다. entailment pair를 positive pair로, contradiction pair를 hard negative pair로 사용한다. 그리고 다른 in-batch instances를 negative로 사용한다. 이 간단한 방식도 좋은 성능을 보여주었고 NLI dataset이 특히 sentence embedding을 학습하는 데 효과적이라는 걸 발견했다.

SimCSE에 대해 이해하기 위해, semantically-related positive pair 간의 alignment(가까이 위치하는 정도)와 전체 representation space의 uniformity(균일하게 분포되어 있는지)를 분석했다. 경험적으로 unsupervised SimCSE는 alignment를 감소시키지 않고 uniformity를 향상시켰다. 그리고 contrastive learning은 sentence embedding distribution을 flatten해서 anisotropy를 완화시켰다(즉, uniformity를 향상시켰다).

## Background: Contrastive Learning

contrastive learning은 의미적으로 가까운 representation을 가깝게 하고 먼 representation을 밀어내는 걸 학습하는 방법이다. N pair를 갖는 mini-batch에서의 training objective는 다음과 같다.

![1](../blogImage/32-1.png)

$h, h^+$는 $x, x^+$의 representation이다. 논문에서는 input sentence들을 pre-trained LM으로 인코딩한 후($h=f_{\theta}(x)$) contrastive learning objective(1)을 통해 fine-tuning한다.

**Positive instances**

($x_i, x_i^+$) pair를 어떻게 만드는지 또한 중요하다. 비전에서는 하나의 이미지를 2개로 random transformation(e.g. cropping, flipping, rotation 등)한다. language representation에서는 최근 word deletion, reordering, substitution 등의 augmentation 기술이 사용되었다. 그러나 언어의 discrete한 특성 때문에 data augmentation은 본질적으로 어렵다. 뒤에서 설명하겠지만, augmentation 대신 단순히 standard dropout을 사용하는 것 만으로 더 좋은 성능을 낼 수 있었다.

비슷한 contrastive learning objective가 연구되긴 했었는데 여기서는 ($x_i,x_i^+$) pair를 QA pair같은 supervised dataset으로부터 구했고 discrete한 특성 때문에 dual-encoder를 사용했다($x_i,x_i^+$ 각각 $f_{\theta_1}, f_{\theta_2}$).

**Alignment and uniformity**

contrastive learning의 key property로 alignment와 uniformity가 있고 이 둘을 이용해 representation의 퀄리티를 측정한다. alignment는 positive pair embedding들 간의 거리를 계산하고 uniformity는 embedding들이 얼마나 균일하게 잘 분포되어 있는지를 측정한다.

![2](../blogImage/32-2.png)

![3](../blogImage/32-3.png)

## Unsupervised SimCSE

여기서의 핵심은 $x_i,x_i^+$ 각각에 다른 dropout mask를 사용해서 positive pair를 만드는 것이다. 단순히 같은 input을 encoder에 2번 넣어주게 되고 각각 다른 dropout mask $z, z^{'}$을 이용해 2개의 embedding을 만든다. 이 과정은 $h_i^z = f_{\theta}(x_i,z)$로 표현되고 N개의 mini-batch에 대해 training objective는 다음과 같다.

![4](../blogImage/32-4.png)

**Dropout noise as data augmentation**

위 방식을 minimal form of data augmentation으로 볼 수 있다. 즉, 동일한 sentence를 이용하고 dropout mask만 다르게 사용한 embedding을 구한다. 이 방식을 STS-B dev set에 대해 다른 training objective들(deletion, replacement 같은 data augmentation 기술. $h=f_{\theta}(g(x),z)$로 표현됨)과 비교했다. 

![5](../blogImage/32-5.png)

그 결과 dropout noise를 사용한 방식(SimCSE)이 가장 성능이 좋았다.

unsupervised SimCSE의 training objective와 NSP를 single encoder, dual-encoder 각각에 대해 비교했다.

![20](../blogImage/32-20.png)

NSP보다는 SimCSE가 더 성능이 좋았고 dual-encoder보다는 single-encoder가 성능이 좋았다.

**Why does it work?**

dropout noise의 역할을 이해하기 위해, 여러 dropout rate로 실험했고 $p=0.1$일 때 가장 성능이 좋은 걸 발견했다. 

![6](../blogImage/32-6.png)

2가지 특이한 흥미로운 케이스를 발견했는데, $p=0.0$일 때(no dropout)와 "fixed 0.1"일 때(pair에 동일한 0.1 dropout mask를 사용하는 경우)이다. 이 두가지의 경우 embedding pair는 같은 값을 가지고 이는 상당한 성능 감소를 일으킨다. alignment와 uniformity에 대해 학습 10 step마다 visualize한 결과가 Figure 2이다.

![7](../blogImage/32-7.png)

pre-trained checkpoint에서 시작해서 모든 모델의 uniformity는 상당히 향상되었다. 그러나 2가지 특이 케이스는 alignment가 감소했다. 반면 unsupervised SimCSE는 dropout noise 덕분에 alignment를 유지할 수 있었다. "delete one word" augmentation model은 alignment의 성능이 좋았고 uniformity와 약간 향상되었지만 결과적으로 unsupervised SimCSE의 성능이 더 좋았다.

## Supervised SimCSE

supervised NLI dataset에서 entailment를 통해 직접적으로 ($x_i,x_i^+$) pair를 구하고 식(1)을 이용해 optimize한다.

**Choices of labeled data**

먼저 어떤 supervised dataset이 positive pair ($x_i,x_i^+$)를 만들기 적합한지 찾아보았다. 아래 표의 여러 dataset을 이용해 contrastive learning model을 학습시킨 후 결과를 비교해보았다.

![8](../blogImage/32-8.png)

NLI dataset에서 entailment pair를 사용한 것이 가장 성능이 좋았다. 여기서 두 문장간의 lexical overlap(2개의 BOW간의 F1 score)이 낮았기 때문이라고 보는데 NLI의 entailment pair에서는 39%밖에 안 되었지만 QQP에서는 60%, ParaNMT에서는 55%나 되었다.

**Contradiction as hard negatives**

NLI dataset에서 contradiction pair를 hard negative로 사용했다. NLI dataset을 만들 때 annotator는 entailment, neutral, contradiction 3가지 경우에 대해 작성하기 때문에 positive pair가 있으면 항상 hard negative pair가 하나씩 있다. 

![9](../blogImage/32-9.png)

Figure (1)-right

($x_i,x_i^+$)를 ($x_i,x_i^+, x_i^-$)로 확장하고 training objective  $ℓ_i$는 다음과 같이 수정된다.

![10](../blogImage/32-10.png)

Table 4의 결과에 나타나듯, hard negative를 더해줌으로써 84.9 → 86.2로 한층 향상됨을 볼 수 있다. 여기까지가 최종 supervised SimCSE이다. 

ANLI dataset을 더해보고 unsupervised SimCSE와 결합하는 방법을 시도해보기도 했지만 의미있는 성능 변화는 없었다. dual-encoder supervised SimCSE는 오히려 성능을 감소시켰다(86.2 → 84.2).

## Connection to Anisotropy

![11](../blogImage/32-11.png)

최근 연구에서는 language representation이 anisotropy 문제(학습된 embedding이 vector space의 좁은 corn 부분만 차지하는 문제)가 있다는 걸 발견했다. autoencoder 같이 input과 output embedding이 동일하게 학습된 LM에서는 anisotropic word embedding을 갖게 되고 pre-trained contextual representation에서는 더 심하다.

이 문제를 해결하는 방법으로는 dominant principal component를 없애기, embedding을 isotropic distribution에 매핑하기 등의 post-processing 방법이 있지만, 여기서 contrastive learning이 anisotropy 문제를 완화시킬 수 있다는 걸 보여주려 한다.

embedding이 vector space 상에서 고르게 분포되어야 한다는 점을 강조하는 측면에서 anisotropy와 uniformity는 자연스레 연관되어 있다. 직관적으로, contrastive learning objective가 negative instance들을 멀리 떨어트려 놓기 때문에 uniformity가 향상된다(즉, anisotropy 문제가 완화된다). 

![12](../blogImage/32-12.png)

contrastive learning objective는 위와 같이 표현되는데, 첫번째 term은 positive instance를 similar하게, 두번째 term은 negative pair를 apart하게 떨어트려둔다.

post-processing 방법은 representation이 isotropic하도록 하는데 목표를 두지만, contrastive learning은 positive pair들의 alignment도 최적화하는 효과가 있다. 

## Experiment

### Evaluation Setup

7가지 semantic textual similarity(STS) task에서 실험했다. 

**Training details**

BERT(uncased)와 RoBERTa(cased)의 pre-trained checkpoint에서부터 시작했고 [CLS] representation을 sentence embedding으로 여겼다. unsupervised SimCSE는 $10^6$개의 랜덤 샘플링한 english wikipedia sentence를 이용했고, supervised SimCSE는 MNLI와 SNLI를 이용해서 학습했다.

### Main Results

unsupervised SimCSE와 supervised SimCSE를 STS task에서 이전의 sota sentence embedding method와 비교했다. unsupervised baseline으로는 GloVe, average BERT or RoBERTa embedding(첫번째 layer와 마지막 layer의 평균을 이용하는 게 마지막 layer의 값만 사용하는 것 보다 좋다는 최근 결과가 있다.), BERT-flow와 BERT-whitening 같은 post-processing method들이 있다. 또한, contrastive objective를 사용하는 다른 method들과 비교도 진행했는데, 다음과 같다.

1. IS-BERT : global, local feature 간의 일치를 최대화 함.
2. DeCLUTR : 같은 document에서 다른 span을 positive pair로 취급함.
3. CT : 한 문장을 다른 두 encoder로부터 구한 embedding을 align함.

supervised baseline에는 InferSent, Universal Sentence Encoder, SBERT/SRoBERTa와 post-processing method들(BERT-flow, BERT-whitening, CT)이 있다.

![13](../blogImage/32-13.png)

SimCSE는 NLI supervised data의 여부에 상관 없이 모든 dataset에서 상당한 성능 향상을 보였다. 그리고 아래의 transfer task 성능 결과를 보면 SimCSE가 기존 연구들과 같거나 더 나은 성능을 보인다는 걸 알 수 있다. 그리고 MLM objective를 추가하는 것이 성능 향상에 도움이 되었다.

![14](../blogImage/32-14.png)

### Ablation Studies

다른 pooling method들과 hard negative들의 영향에 대해 연구했다. 

**Pooling methods**

이전 연구에서 pre-trained 모델의 average embedding(첫번째 layer와 마지막 layer의 평균)을 사용하는 게 [CLS]를 사용하는 것보다 더 성능이 좋다는 것이 밝혀졌다. 다음 표는 unsupervised/supervised SimCSE에서 다른 polling method를 사용했을 때 비교한 표이다. [CLS] representation을 만들 때 기존 BERT에서는 마지막에 MLP layer를 거친다. 여기서, MLP layer가 있을 때와 없을 때, 훈련 동안에만 있게 했을 때, 이렇게 3가지 경우와 average embedding을 비교했다.

![15](../blogImage/32-15.png)

unsupervised SimCSE에서는 MLP와 함께 [CLS] representation을 사용한 방법이 가장 성능이 좋았고 supervised SimCSE에서는 다른 pooling method들이 별 차이 없었다. 따라서 디폴트로 unsupervised SimCSE에서는 [CLS] with MLP(train)을, supervised SimCSE에서는 [CLS] with MLP를 사용했다.

**Hard negatives**

직관적으로 봤을 때, hard negative들(contradiction examples)은 in-batch negative들과 똑같게 여기는 것 보다 다르게 하는 게 성능상 더 나아보인다. 따라서 training objective를 다음과 같이 $i=j$일 때 $\alpha$위의 지수부분이 1이 되고 아니면 0이 되는 식을 추가해주었다.

![16](../blogImage/32-16.png)

그리고 contradiction 뿐만 아니라 neutral도 hard negative로 여기고 실험해보았다.

![17](../blogImage/32-17.png)

$\alpha$ = 1일 때 가장 성능이 좋았고, neutral hypotheses는 별다른 이점이 없었다.

## Analysis

**Uniformity and alignment**

![18](../blogImage/32-18.png)

여러 sentence embedding model들의 alignment와 uniformity 값을 나타낸 figure이다. 일반적으로 uniformity와 alignment에서 둘 다 좋은 결과를 보인 모델이 최종 성능도 더 좋다(색깔을 통해 알 수 있다). 실험 결과는 다음과 같다.

1. pre-trained embedding은 좋은 alignment을 갖지만 uniformity가 좋지 않다. 즉 anisotropic하다.
2. BERT-flow나 BERT-whitening 같은 post-processing method는 uniformity를 향상시켜 주지만 alignment가 감소한다.
3. unsupervised SimCSE는 좋은 alignment를 유지한 채로 pre-trained embedding의 uniformity를 향상시킨다.
4.  SimCSE에 supervised data를 사용하면 alignment가 개선된다.

**Qualitative comparison**

SBERT_base와 SimCSE-BERT_base를 이용해 작은 규모의 retrieval 실험을 진행했다. Flickr30k dataset에서 150k개의 caption을 이용해 유사한(cosine similarity) 문장을 찾게끔 했고 그 결과는 다음과 같다.

![19](../blogImage/32-19.png)

결과적으로 SimCSE를 이용해 검색한 문장이 더 퀄리티가 좋았다.

## Conclusion

STS task에서 sota sentence embedding을 개선시킨 simple contrastive learning framework인 SimCSE를 제안한다. 두가지 접근법을 제안했는데, unsupervised approach는 dropout을 통해 noise를 주는 방식이었고 supervised approach는 NLI dataset을 이용한 방식이었다. uniformity와 alignment metric을 기준으로 이 접근법들의 다양한 분석을 했고, contrastive learning 특히 unsupervised SimCSE가 NLP 분야에 널리 쓰이기를 기대한다.

---

피드백은 언제나 환영합니다. 잘못된 내용이 있으면 댓글로 피드백 부탁드립니다.
