---
id: "20230323"
slug: "/20230323"
title: "Attention-over-Attention Neural Networks for Reading Comprehension"
description: ""
author: "Vive Kang"
date: "2023-03-23"
image: ""
tags: ["NLP", "paper review"]

---
## Motivation & Introduction

cloze test는 context 정보를 고려하여 주어진 문장의 빈칸에 알맞은 단어를 예측하는 것이다.

논문에서는 ‘attention-over-attention’ 모델이라는 새로운 NN 아키텍쳐를 소개한다. 이름에서 알 수 있듯, 기존의 document-level attention 위에 또 다른 어텐션을 두는 것이다. 

our model could automatically generate an "attended attention" over various document-level attentions, and make a mutual look not only from query-to-document but also document-to-query, which will benefit from the interactive information.

이 논문의 main contribution을 요약하자면 다음과 같다.

1. 기존 어텐션 위에 또 다른 어텐션이 있는 메커니즘을 처음 제안한다.
2. 복잡한 아키텍쳐를 이용하거나 학습할 수 없는 hyperparameter들을 사용했던 이전 모델과는 다르게, 훨씬 단순하면서도 성능이 좋은 모델을 소개한다.
3. N-best re-ranking strategy를 소개한다.

## Cloze-style Reading Comprehension

### Task Description

일반적인 Cloze-style reading comprehension은 다음 3가지로 표현가능하다. document $D$, query $Q$, answer to the query $A$. answer는 document 안에 있는 single word 이다. answer는 단순히 전치사를 예측하는 것에서 named entity를 구체적으로 예측하는 것까지 다양하게 나타날 수 있다.

### Existing Public Datasets

NN을 학습하기 위해선 많은 양의 데이터가 필요한데, cloze-style reading comprehension을 위한 dataset이 몇 가지 있다.

**CNN / Daily Mail**

이 dataset은 뉴스 기사 데이터로, 기사 본문을 document, document를 요약한 내용을 query로 사용한다. 그리고 query에서 하나의 entity를 빈칸으로 뚫기 위해 special placeholder로 대체하고 그 빈칸 부분이 answer가 된다. named entity의 수가 한정적이어서 일반적인 지식에 대한 내용보다는 entity간의 관계에 초점을 둔다.

**Children’s Book Test (CBTest)**

20문장을 document로 활용하고 21번째 문장을 query로 활용한다. query에서 한 단어를 빈칸으로 만들어 answer 사용한다. Named Entity(NE), Common Nouns(CN), 동사, 전치사 이렇게 4가지 종류를 빈칸으로 이용할 수 있다. 동사와 전치사는 document 내용에 영향을 덜 받고 특히 전치사가 빈칸일 때, 사람들은 document가 굳이 없어도 문제를 잘 풀었다. 따라서 NE, CN에 초점을 둔다.

## Attention-over-Attention Reader

cloze-style training set $D, Q, A$가 주어졌을 때, AoA Reader 모델은 다음 순서대로 아키텍쳐를 구성한다.

### 1. Contextual Embedding

document, query의 각 단어를 one-hot encoding 후, shared embedding weight matrix $W_e$를 각 단어에 곱해줘서 embedding한다. shared weight를 쓰면 document, query를 둘 다 이용해서 embedding을 할 수 있다. 
bi-directional GRU를 document, query에 각각 적용해서 각 단어의 contextual representation을 얻는데, 양방향의 representation을 concat해서 사용한다. 

![0](../blogImage/13-0.png)

결과적으로, $|D|\times 2d$의 $h_{document}$, $|Q| \times 2d$의 $h_{query}$ 값을 얻는다($d$ 는 embedding 차원).

### 2. Pair-wise Matching Score

document의 i번째 단어와 query의 j번째 단어의 matching score는 dot-product로 표현된다.

$$
M(i,j)=h_{document}(i)^{\top}\cdot h_{query}(j)
$$

각각의 document와 query 단어들에 대해 $M_{|D|\times |Q|}$ matrix를 만든다.

### 3. Individual Attentions

$M_{|D|\times |Q|}$에서 각 query 단어에 대한 document-level attention을 얻기 위해 column-wise softmax를 취한다. 즉, query의 t번째 단어에 대해 다음과 같이 document-level attention $\alpha(t)$를 구할 수 있다.

![1](../blogImage/13-1.png)

$\alpha_{|D|\times |Q|}$는 **query-to-document attention**으로 볼 수도 있다.

### 4. Attention-over-Attention

개별 어텐션들을 하나의 final attention으로 만들 때 기본적인 더하기나 평균내는 방법을 쓰는 게 아니라, 개별 어텐션들의 중요도를 매기는 새로운 어텐션을 거친다.

![2](../blogImage/13-2.png)

먼저, t번째 document word에 대해 어떤 query word가 중요한지에 대한 "importance" 분포를 계산하려 한다. matrix $M$에 이번엔 row-wise softmax를 취해 query-level attention을 구한다. query-level attention $\beta(t)$는 **document-to-query attention**이며 다음과 같이 표현된다.

![3](../blogImage/13-3.png)

지금까지 query-to-document attention $\alpha$, document-to-query attention $\beta$를 구했다(이전까지의 연구는 query-to-document attention만 사용했었다). 이제, $\beta(t)$를 모든 context word t에 대해 평균을 낸다(이미 각각이 softmax 값이기 때문에 바로 평균을 내어도 normalize가 깨지지 않는다).

![4](../blogImage/13-4.png)

$\beta$는 개별 context word들이 갖는 모든 query word에 대한 중요도를 평균낸 어텐션 값이다. context가 전반적으로 어떤 query word에 높은 중요도를 매기는 지에 대한 의미를 갖고 있다.

마지막으로, $\alpha$와 $\beta$의 dot-product를 수행해서 "attended document-level attention" $s_{|D|}$를 구한다. 좀 더 직관적으로, 개별 query word가 갖는 document-level attention의 weighted sum을 수행하는 것이고, query를 고려했을 때 document의 각 단어가 어느정도의 중요도를 갖는 지에 대한 의미를 담고 있다.

$$
s = \alpha^{\top}\beta
$$

이 방식을 통해 각 query word가 갖는 중요성(영향력?)을 고려하여 attended document-level attention을 만들 수 있다.

### 5. Final Predictions

sum attention 메커니즘을 이용한다. 위 그림의 "Mary" 예시를 보면 이해가 잘 된다.
전체 vocabulary $V$와 document $D$에서 단어 $w$가 나타나는 위치 값 $I(w,D)$에 대해 최종 빈칸에 들어갈 단어에 대한 확률은 다음과 같다.

![5](../blogImage/13-5.png)

앞서 설명한 dataset들은 named entity, common noun을 예측을 하기 때문에, 최종 output의 P확률이 높은 document 내의 단어를 예측 값으로 본다.

## N-best Re-ranking Strategy

보통 cloze-style reading comprehension을 할 때, 처음 예측한 정답 값이 정확한지, 문법적으로 올바른지 등을 double-check 하고, 문제가 있다면 그 다음으로 확률이 높은 단어를 선택하는 걸 반복하는 식으로 진행된다.
이 방법에서 착안하여, NN에서 정답을 예측한 후에 수행할 N-best re-ranking strategy을 다음과 같이 제안한다.

**1. N-best Decoding**

예측 값을 decoding할 때, 가장 확률이 높은 한 단어만 디코딩하는 것이 아니라 확률 높은 N개의 단어(N-best)를 디코딩한다.

**2. Refill Candidate into Query**

N개의 후보 단어들을 각각 빈칸에 채워넣어서 candidate sentences를 만든다.

**3. Feature Scoring**

candidate sentence를 다양한 측면에서 점수를 매길 수 있다.

1. Global N-gram LM
얼마나 fluency(정확한 뜻에 상관없이 문장이 자연스럽게 말이 되는지를 나타내는 지표)한 문장인지 N-gram으로 평가한다. training data의 document 부분을 가지고 학습한다.
2. Loca N-gram LM
Global과는 다르게, document 내의 정보에 초점을 두고 test data의 document를 학습한다. 전체 test set에 대해 학습하는 게 아니라 각각의 샘플 데이터를 따로 학습한다. 모르는 단어가 많을 때 유리하다.
>> 모르는 단어가 많을 때 왜 유리한지?
3. Word-class LM
1번과 비슷한데, 단어에 대한 clustering class들이 있을 때 각 단어를 class ID로 바꿔서 이용한다.

**4. Weight Tuning**

위 3가지 feature들의 weight를 튜닝하기 위해, K-best MIRA 알고리즘을 사용했다.

- K-best MIRA algorithm
    
    매번 k개의 best 단어들 뽑고 각각을 실제 정답과 비교해서 weight를 학습시키는 방식. 정해진 횟수만큼 진행하거나 convergence할 때까지 진행한다.
    
    여기서는 validation set을 이용해 weight 튜닝을 했다.
    

**5. Re-scoring and Re-ranking**

각 feature의 weight를 구한 후, N-best sentence에 대해 각 feature의 weighted sum을 구하고, 그 값(cost)이 가장 낮은 문장의 단어를 최종 답으로 선택한다.

## Experiments

### Experimental Setups

![6](../blogImage/13-6.png)

CNN, CBTest NE/CN 에 대한 통계 

![7](../blogImage/13-7.png)

Hidden Layer: GRU 내부 layer

re-ranking 단계에서 K 값을 잘 고르는 것이 크게 중요하지 않다는 것을 발견해서 K=5, 5-best를 이용했다.

### Overall Results

![8](../blogImage/13-8.png)

AoA Reader 단일 모델은 이전의 best 앙상블 모델과 성능이 유사하다. Reranking을 이용했을 때 성능이 증가했다. 

attention-over-attention의 성능을 확인하기 위해, CAS Reader(개별 어텐션들을 하나의 final attention으로 만들 때 기본적인 더하기나 평균내는 방법을 쓰는 모델)와 비교했는데 성능이 훨씬 좋더라.

### Effectiveness of Re-ranking Strategy

re-ranking에서 문장을 평가할 때 어떤 지표가 영향을 많이 미치는지 알아봤다.

![9](../blogImage/13-9.png)

![12](../blogImage/13-12.png)

NE는 CN에 비해 local feature에 영향을 많이 받았는데, local에서는 common noun보다 특정 named entity의 비율이 높기 때문에 당연하다. 반대로, common noun을 맞출 때는 local 정보가 상대적으로 덜 중요했다.

## Quantitative Analysis

document의 길이와 정확도에 대한 연구를 진행했다.(Figure 2)

![10](../blogImage/13-10.png)

![11](../blogImage/13-11.png)

AS Reader와 비교했을 때, 항상 정확도가 높았고 특히 길이가 700이 넘어가면서 성능이 훨씬 좋아졌다. AoA Reader가 긴 doument를 다루는 능력이 더 뛰어났다.

## Conclusion

cloze-style reading comprehension task를 위해 attention-over-attention reader 라는 새로운 아키텍쳐를 제안했다. document에 대한 어텐션 뿐만 아니라 query에 대해서도 진행해 상호간의 이점을 얻게끔 했다.


---

피드백은 언제나 환영합니다. 잘못된 내용이 있으면 댓글로 피드백 부탁드립니다.
