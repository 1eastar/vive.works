---
id: "20230408"
slug: "/20230408"
title: "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction"
description: ""
author: "Vive Kang"
date: "2023-04-08"
image: ""
tags: ["NLP", "paper review"]

---
## Motivation & Introduction

사람이 text를 읽을 땐 그에 대한 이해를 바탕으로 commonsense inference를 한다. 기계가 이 능력을 얻기 위해선 무한한 상황 속에서 관련있고 정확한 commonsense를 파악할 수 있어야 한다. 이 논문에서는 commonsense를 얻고 파악하는 걸 knowledge base construction으로 대신 확인하고, 자동으로 commonsense KB를 만들기 위해 large-LM이 knowledge를 생성하도록 잘 학습되는 지 확인해보려 한다.

commonsense knowledge는 기존의 2개의 entity와 그 사이의 특정한 "known" relation을 갖는 schema를 완벽히 적용할 수 없고, natural language phrases인 entity와,  그 entity들을 잇는 "any" concept인 relation을 이용하는 접근법이 필요하다.
최근 deep contextualized language model을 학습시키는 것에 대한 발전은 extractive method 그 이상을 보여주었고 commonsense KB construction로 가는 길을 열어주었다. 논문에서는 기존의 knowledge set tuples를 이용해 학습해서 commonsense KB를 만드는 Commonsense Transformer(COMET)을 제시한다. 

논문의 contribution은 다음과 같다. (1) knowledge base construction에 대한 general한 방식을 만들었다. 모델은 새로운 node를 만들고 기존 node들 간의 edge를 파악해야 한다. 모델은 기존의 seed phrase와 relation type을 정확히 생성해내면서 학습된다. (2) large-scale transformer LM을 이용해 commonsense knowledge를 생성하도록 학습시키기 위한 framework를 개발했다. (3) ATOMIC과 ConceptNet에 대해 우리의 방식으로 만든 commonsense knowledge의 quality, novelty, diversity에 대해 연구했다. 그리고 어느 정도의 seed tuples가 knowledge model을 학습시키기 효율적인 지에 대한 연구도 했다.

## Learning to Generate Commonsense

COMET은 LM을 seed set of knowledge tuples로 학습시켜서 commonsense knowledge bases를 만드는 프레임워크이다. tuples들은 KB structure와 relation의 형태로 주어지고, COMET은 pre-trained LM representation을 이용해 새로운 node와 edge를 seed KG에 추가하는 방법을 학습한다.

### Task

COMET에게는 학습을 위한 \{ s, r, o \} 형태의 KB tuple이 주어진다. 여기서 task는 s, r이 주어졌을 때 o를 생성하는 것이다.

**Notation**

$X^i=\{x_0^i,...,x_{|i|}^i\}, i=s,r,o$ 

각 s, r, o phrase마다 위와 같이 각 토큰으로 표현한다. 

### Transformer Language Model

transformer기반의 GPT 아키텍쳐를 이용했다. 

![0](../blogImage/22-0.png)

(b)에 나와있는 구조대로, 각 layer는 구조적으로 동일한 transformer block을 사용했다(학습 parameter만 다르다). 하나의 block 내의 연산 과정은 다음과 같다(기존 transformer와 동일).

![1](../blogImage/22-1.png)

**Multi-headed Attention**

multi-head attention의 구조는 기존 transformer의 구조와 동일하다(a).

(c)를 보면 점선 대각선 화살표가 있는데, 이는 현재 time step의 output을 다음 layer의 미래 time step에서 사용한다는 의미이다. key, value 값을 이렇게 이전 layer의 앞선 time step으로부터 받아오고, query는 같은 time step에 이전 layer로부터 받아온다.

![2](../blogImage/22-2.png)

**Input Encoder**

모델에 input으로 knowledge tuple \{ s, r, o \}를 concat해서 입력한다($X=\{X^s,X^r,X^o\}$).

self-attention에는 각 토큰의 순서(위치) 정보가 없기 때문에 positional embedding $p_t$으로 absolute positional embedding으로 initialize한 값을 사용한다. 따라서 각 토큰에 대해 이렇게 표현된다.

$$
h_t^0=e_t+p_t
$$

t는 time step, $h^0$은 첫번째 transformer layer로의 input 값임을 뜻한다.

## Training COMET

s, r phrase의 토큰들을 concat한 후([$X^s,X^r$]) $X^o$ 토큰들을 생성하도록 학습된다.

**Loss Function**

다음과 같이 object 토큰 $X^o$를 예측하는 conditional loglikelihood를 최대화하도록 학습된다.

![3](../blogImage/22-3.png)

![4](../blogImage/22-4.png)

여러 training task들에서 s, r, o 토큰을 구성하는 방식을 보여준다.

**Datasets**

ATOMIC과 ConceptNet을 knowledge seed set으로 사용했다.

**Initialization**

GPT의 최종 weight들로 initialize했다. 추가적인 special token을 fine-tuning vocabulary에 추가했다(ATOMIC에서 oReact, ConceptNet에서 IsA 같은 relation embedding들).

**Hyperparameters**

GPT처럼, 12-layer, 768 dimension hidden state, 12 attention heads.

dropout 0.1, GeLU, batch size 64.

## ATOMIC Experiments

ATOMIC dataset은 877K tuples로 구성되어 있고 특정 event에 대한 prompt로 commonsense knowledge가 구성되어 있다. ATOMIC은 전체 commonsense를 9개의 dimension으로 분류한다. 논문의 실험에서, event("X goes to the store")가 subject phrase $s$이고 dimension("xIntent")이 relation phrase $r$, causes/effects("to get food")가 object phrase $o$이다.

### Setup

**Metrics**

BLEU-2를 자동 평가 지표로 모델을 평가하고 gold generation에 대한 perplexity도 보고했다. 그리고 생성된 모든 tuples에 대해 새로운 tuple인 비율(% N/T sro), 새로운 object인 비율(% N/T o)도 측정했다. 그리고 새로운 object마다 등장하는 횟수가 있을텐데 단 한번 등장한 object의 비율(% N/U o)를 측정했다. 마지막으로 human evaluation도 수행했다.

**Baselines**

LSTM seq-to-seq 모델을 이용해 encoding하는 모델들을 baseline으로 두었다.

**Ablations**

pre-trained 모델을 사용할 때의 효과를 알아보기 위해 pre-trained weight로 initialize하지 않은 버전의 COMET을 만들었다( COMET(-pretrain) ). 그리고 효율적인 학습 데이터의 양을 알아보기 위해 training data의 비율을 조절해서 평가했다. 마지막으로, decoding scheme에 따라 candidate knowledge tuple의 퀄리티가 달라지는 지 연구했다. (1) argmax greedy decoding, (2) beam search(b=2,5,10), (3) top-k sampling(k=5), 3가지의 decoding method에 대해 human evaluation을 수행했다.

### Results

**Overall performance**

![5](../blogImage/22-5.png)

COMET이 모든 baseline보다 성능이 좋았다. 성능 뿐만 아니라 baseline보다 새로운 object를 더 잘 만들어냈다.

![6](../blogImage/22-6.png)

**Learning knowledge from language**

pre-trained 모델을 이용해 initialize하는 것도 상당히 중요했다. Table 2의 **Avg**(human evaluation)을 보면 상대적으로 14% 가량 성능이 좋아졌다. GPT 모델로부터 학습한 language representation이 commonsense knowledge를 생성하는 task에 transfer 될 수 있다는 걸 보여준다.

**Effect of decoding algorithm**

![7](../blogImage/22-7.png)

여러 generation policy가 knowledge quality에 미치는 영향을 나타낸 표이다. 

candidate 수가 늘어날 수록 성능이 감소했다. 가장 흥미로운 점은 greedy decoding이 human eval과 10% 정도밖에 차이가 안 난다는 점이다. 따라서 COMET을 human evaluator와 함께 사용한다면 더 효과적일 것이라 생각한다.

**Efficiency of learning form seed tuples**

모든 domain이 많은 양의 commonsense KB 데이터를 갖고 있지는 않기 때문에, training에 사용된 데이터의 양이 생성된 knowledge의 quality와 novelty에 얼마나 영향을 미치는 지 연구했다.

![8](../blogImage/22-8.png)

10%의 데이터만 사용해도 충분히 괜찮은 결과를 생성해냈다. 그리고 pre-trained 모델 없이 full data training을 한 것과 10%의 데이터만 사용한 것의 결과가 거의 비슷했다. 이를 통해 pre-trained 모델의 중요성을 또 한번 알 수 있다고 말한다.

## ConceptNet Experiments

ConceptNet의 tuple도 sro 형태를 가진다. 34개의 relation을 갖는 100K 버전의 training set이 모델을 학습시키는 데 사용되었다.

### Setup

ConceptNet relation을 생성하는 것에 대한 지표로 gold relation의 perplexity를 평가한다. 생성된 knowledge의 퀄리티를 평가하기 위해 생성된 positive example(정답이 50% 이상의 확률인 경우)의 수를 측정했다. novelty를 평가하기 위해 ATOMIC experiments처럼 N/T sro, N/T o를 이용했다.

**Baselines**

Bi-LSTM을 살짝 변형시켜서 사용한다. 이 모델은 양방향(sr → o,  or → s)으로 knowledge를 학습하지만 여기서는 sr → o만 평가한다.

**Ablations**

1. COMET(-pretrain)
2. relation들을 natural language로 매핑해서(IsA → is a) 각각의 relation을 special embedding으로 학습하는 것이 아닌 language representation으로 학습하도록 함.
3. ConceptNet을 사용했지만 relation을 자연어로 매핑하지 않은 버전(COMET -RELTOK)도 만듦

### Results

![9](../blogImage/22-9.png)

**Quality**

COMET의 95.25%의 classifier score는 대부분의 경우 생성된 tuple이 정답이었다는 걸 뜻한다. adversarial generation 덕분에 score가 높게 나왔다고 하는데, 어느 부분이 adversarial인지 잘 모르겠다.

![10](../blogImage/22-10.png)

**Novelty**

59.25%의 새로운 tuple을 만들어냈고, 이는 모델이 node들 간의 새로운 edge를 만들어 낼 수 있고 심지어 새로운 node도 만들어 낼 수 있다는 걸 보여 준다. 한 가지 단점으로는 기존 training set에 있던 tuple의 simplified 버전을 만들어 냈다는 점이다. 그래도 많은 tuple들이 training set의 tuple들과 관련 없는 완전히 새로운 tuple이었다.

training set과 development set에서 같은 s, r에 대해 object가 얼마나 다른 지 비교하기 위해 edit distance를 계산해봤다. edit distance가 1이면 sequence 전체가 다른 word token이고, 0이면 같은 sequence를 뜻한다.

아래 Figure 4를 보면 75% 이상의 새로운 tuple들이 edit distance 0.5 이상인 object를 가지는데, 이는 대부분의 새로운 tuple object가 기존 training set의 object들과 상당히 다르다는 걸 뜻한다.

![11](../blogImage/22-11.png)

**Learning knowledge from language**

ATOMIC에서와 같은 방식으로 COMET(-pretrain)과 비교했을 때 성능이 훨씬 좋았다.(Table 6)

**Representing relations with language**

Table 6에서, relation들을 자연어로 매핑한 COMET-RELTOK과의 비교를 해보면 automatic score가 거의 같지만, 구체적인 예시들을 보면 relation을 자연어로 매핑했을 때의 이점이 있다는 insight를 얻을 수 있다. 예를 들어, ConceptNet training set에는 "dove"에 대한 비조류학적 레퍼런스로 오직 "dove CapabelOf fly"라는 내용 밖에 없다. 자연어 relation을 사용하는 모델의 경우 "dove SymbolOf purity"처럼 "dove"라는 단어에 대해 더 넓은 의미를 이해하고 일반화할 수 있었다. 반면, relation을 그대로 사용한 경우, "dove SymbolOf submarine"이라는 "dove"와는 관계 없는 "submarine"이라는 단어로 tuple을 만들어 낸다.

## Conclusion

COMET은 commonsense KB를 automatic construction하기 위한 프레임워크이다. COMET은 LM의 학습된 weight를 이용해 학습하면서 새롭고 다양한 commonsense knowledge tuples를 만들어 낸다.

ATOMIC과 ConceptNet, 이 두개의 commonsense KB에 대해 실험해봤을 때, COMET은 새로운 commonsense knowledge를 만들어냈고 이 knowledge는 사람이 보기에도 correct했다.

---

피드백은 언제나 환영합니다. 잘못된 내용이 있으면 댓글로 피드백 부탁드립니다.
